{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "LoRa_Customer_support_lora_llama_3_2_3b_instruct",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shameer-phy/GenAI/blob/main/FineTuning%20/LoRa_Customer_support_lora_llama_3_2_3b_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing necessary libraries"
      ],
      "metadata": {
        "id": "TyQJSFWpA_6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* transformers: Provides state-of-the-art pretrained models for NLP, computer vision, and beyond.\n",
        "* datasets: A library for easy access to a wide range of datasets for ML and NLP tasks.\n",
        "* accelerate: Simplifies distributed training and inference for PyTorch models.\n",
        "* torch: PyTorch library for building and training deep learning models.\n",
        "* bitsandbytes: Optimized GPU quantization and acceleration for large-scale models.\n",
        "* peft: Parameter-efficient fine-tuning techniques for large language models.\n",
        "* trl: Tools for training transformer models with reinforcement learning techniques."
      ],
      "metadata": {
        "id": "eKCYFhbCBBJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes trl datasets peft tokenizers huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:03.550317Z",
          "iopub.execute_input": "2025-02-03T14:42:03.550636Z",
          "iopub.status.idle": "2025-02-03T14:42:06.885015Z",
          "shell.execute_reply.started": "2025-02-03T14:42:03.550607Z",
          "shell.execute_reply": "2025-02-03T14:42:06.884204Z"
        },
        "id": "2qeK4eAVebtm",
        "outputId": "050df48c-a8e7-480f-f6c8-107d242cef3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.1)\nRequirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.21.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers==4.47.1 accelerate==0.34.2 bitsandbytes==0.45.0 trl==0.13.0 datasets==3.2.0 peft==0.14.0 tokenizers==0.21.0 huggingface_hub==0.26.0"
      ],
      "metadata": {
        "id": "sBfOIQkrnQQY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:06.886266Z",
          "iopub.execute_input": "2025-02-03T14:42:06.886542Z",
          "iopub.status.idle": "2025-02-03T14:42:06.889775Z",
          "shell.execute_reply.started": "2025-02-03T14:42:06.886511Z",
          "shell.execute_reply": "2025-02-03T14:42:06.889162Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "TQVzyOuhBGPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from peft import PeftModel,get_peft_model,LoraConfig, TaskType\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "0AkMx3I9BGG-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:06.891389Z",
          "iopub.execute_input": "2025-02-03T14:42:06.891584Z",
          "iopub.status.idle": "2025-02-03T14:42:31.844256Z",
          "shell.execute_reply.started": "2025-02-03T14:42:06.891567Z",
          "shell.execute_reply": "2025-02-03T14:42:31.843622Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "#import os\n",
        "#hf_token = os.environ(HF_TOKEN)\n",
        "\n",
        "#from google.colab import userdata\n",
        "#hf_token = userdata.get('HF_TOKEN')\n",
        "#login(token = hf_token) # Logging into Hugging Face Hub to access models and other resources"
      ],
      "metadata": {
        "id": "n9P7L6ZzBH4L",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:31.84534Z",
          "iopub.execute_input": "2025-02-03T14:42:31.845552Z",
          "iopub.status.idle": "2025-02-03T14:42:31.858618Z",
          "shell.execute_reply.started": "2025-02-03T14:42:31.845532Z",
          "shell.execute_reply": "2025-02-03T14:42:31.858016Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token = hf_token)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:31.859304Z",
          "iopub.execute_input": "2025-02-03T14:42:31.859532Z",
          "iopub.status.idle": "2025-02-03T14:42:32.196271Z",
          "shell.execute_reply.started": "2025-02-03T14:42:31.859512Z",
          "shell.execute_reply": "2025-02-03T14:42:32.195548Z"
        },
        "id": "-kr4qTBPa8L_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model configurations and Dataset Preparation"
      ],
      "metadata": {
        "id": "XJg9kA5MBPDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface model link: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
      ],
      "metadata": {
        "id": "5WgqYkFrBgXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "#base_model = 'deepseek-ai/DeepSeek-R1'"
      ],
      "metadata": {
        "id": "LBACybdSBOVi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:32.196957Z",
          "iopub.execute_input": "2025-02-03T14:42:32.197178Z",
          "iopub.status.idle": "2025-02-03T14:42:32.200601Z",
          "shell.execute_reply.started": "2025-02-03T14:42:32.197159Z",
          "shell.execute_reply": "2025-02-03T14:42:32.199673Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "#from transformers import AutoModelForCausalLM, AutoModel\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "9a5D_VqNubRG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:32.2015Z",
          "iopub.execute_input": "2025-02-03T14:42:32.201768Z",
          "iopub.status.idle": "2025-02-03T14:42:32.217364Z",
          "shell.execute_reply.started": "2025-02-03T14:42:32.201748Z",
          "shell.execute_reply": "2025-02-03T14:42:32.216632Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure 4-bit quantization settings using the BitsAndBytesConfig class\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enable loading the model with 4-bit precision for reduced memory usage\n",
        "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 (nf4), a quantization format for higher accuracy\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation to balance speed and precision\n",
        "    bnb_4bit_use_double_quant=True  # Enable double quantization for better numerical stability\n",
        ")\n",
        "\n",
        "# Load the pre-trained model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,  # Name of the base model defined earlier\n",
        "    #trust_remote_code=True,\n",
        "    #quantization=bitsandbytes_4bit,\n",
        "    device_map=\"auto\",  # Automatically map model layers to available devices (e.g., GPU/CPU)\n",
        "    quantization_config=bnb_config  # Apply the defined 4-bit quantization configuration\n",
        "    #trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# 1. The use of 4-bit quantization helps in reducing memory requirements while maintaining reasonable performance.\n",
        "# 2. `device_map=\"auto\"` ensures the model layers are automatically distributed across available hardware for efficient loading."
      ],
      "metadata": {
        "id": "z7L4sqYLBn7z",
        "outputId": "538d55b4-baa6-4c8f-a3d1-21277988d616",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:42:32.219725Z",
          "iopub.execute_input": "2025-02-03T14:42:32.219909Z",
          "iopub.status.idle": "2025-02-03T14:45:14.858121Z",
          "shell.execute_reply.started": "2025-02-03T14:42:32.219893Z",
          "shell.execute_reply": "2025-02-03T14:45:14.857309Z"
        },
        "colab": {
          "referenced_widgets": [
            "03776d53e5604a59bb7ab67e5c06c8ac",
            "866805e66fff4414860cc73c32b12e9f",
            "ae8c0f639b824378a35c6571157157ab",
            "a64a4f1db6ab44c8ac2fdf0664fa0ef0",
            "8f3998c2c9b3413aa91530d1f644014d",
            "96d4e36de3484f55887b4560f42fe62a",
            "85e88b690b1d4511b6699c1d549e58d7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03776d53e5604a59bb7ab67e5c06c8ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "866805e66fff4414860cc73c32b12e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae8c0f639b824378a35c6571157157ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a64a4f1db6ab44c8ac2fdf0664fa0ef0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f3998c2c9b3413aa91530d1f644014d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96d4e36de3484f55887b4560f42fe62a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85e88b690b1d4511b6699c1d549e58d7"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "# Set the padding token to the end-of-sequence (eos) token\n",
        "# This ensures compatibility when the model processes inputs with padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Configure the tokenizer to apply padding on the right side of the input\n",
        "# This is often the default for causal language models to ensure alignment during training or inference\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "2yPw626eBrgi",
        "outputId": "f9886f40-d29a-4d75-b26e-26937ce7d243",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:14.859806Z",
          "iopub.execute_input": "2025-02-03T14:45:14.860183Z",
          "iopub.status.idle": "2025-02-03T14:45:16.779547Z",
          "shell.execute_reply.started": "2025-02-03T14:45:14.860147Z",
          "shell.execute_reply": "2025-02-03T14:45:16.778808Z"
        },
        "colab": {
          "referenced_widgets": [
            "f766f6b97dff435990e85605da1894ad",
            "7b8b3e713ca748608a02b187bd36fb7e",
            "9bff34d36a0144d0872e150091941efd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f766f6b97dff435990e85605da1894ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b8b3e713ca748608a02b187bd36fb7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bff34d36a0144d0872e150091941efd"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset link: https://huggingface.co/datasets/Victorano/customer-support-1k"
      ],
      "metadata": {
        "id": "m-Ug_EzRBvZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the 'Customer_support_faqs_dataset' from the Hugging Face dataset repository\n",
        "dataset = load_dataset(\"Victorano/customer-support-1k\", split=\"train\")\n",
        "dataset = dataset.remove_columns(['flags', 'category','intent','text'])\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "0ZBBd65fBuRL",
        "outputId": "8755d529-cbce-4a77-a3d8-78af51f13883",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:16.780388Z",
          "iopub.execute_input": "2025-02-03T14:45:16.780607Z",
          "iopub.status.idle": "2025-02-03T14:45:19.2667Z",
          "shell.execute_reply.started": "2025-02-03T14:45:16.780587Z",
          "shell.execute_reply": "2025-02-03T14:45:19.265995Z"
        },
        "colab": {
          "referenced_widgets": [
            "9a0455d9fffd49d783b055d39313a268",
            "08728e3979d44a85a41f107b73574892",
            "af37c1d66e6a4656b7d61ebee089faf1"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/471 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a0455d9fffd49d783b055d39313a268"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/648k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08728e3979d44a85a41f107b73574892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af37c1d66e6a4656b7d61ebee089faf1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "jlRZ4DwLy-jJ",
        "outputId": "7cb80ffe-3510-4325-f18a-c4eca26d79b7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:19.267444Z",
          "iopub.execute_input": "2025-02-03T14:45:19.267666Z",
          "iopub.status.idle": "2025-02-03T14:45:19.272516Z",
          "shell.execute_reply.started": "2025-02-03T14:45:19.267644Z",
          "shell.execute_reply": "2025-02-03T14:45:19.271806Z"
        }
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['instruction', 'response'],\n        num_rows: 200\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the instruction that will guide the assistant's behavior for providing customer support answers.\n",
        "\n",
        "instruction = \"\"\"You are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\n",
        "\n",
        "Always prioritize accuracy and clarity in your answers.\n",
        "If the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\n",
        "Provide useful links or references to detailed documentation when appropriate.\n",
        "Use a friendly and professional tone, ensuring the response is easy to understand.\n",
        "If the FAQ does not cover the question, offer an apology and suggest contacting customer support.\n",
        "\"\"\"\n",
        "\n",
        "def template(row):\n",
        "    # Creating a list of message exchanges (system, user, assistant)\n",
        "    row_json = [{\"role\": \"system\", \"content\": instruction }, # System message with the pre-defined instructions\n",
        "               {\"role\": \"user\", \"content\": row[\"instruction\"]}, # User's question from the dataset\n",
        "               {\"role\": \"assistant\", \"content\": row[\"response\"]}] # The assistant's answer from the dataset\n",
        "\n",
        "    # Tokenizing the chat template and storing the result in the 'text' column (without applying tokenization)\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    return row\n",
        "\n",
        "# Applying the template function to each row in the dataset using multi-processing (4 processes in parallel)\n",
        "dataset = dataset.map(template,num_proc= 4)"
      ],
      "metadata": {
        "id": "FmtWmJbpB14y",
        "outputId": "cb58140f-6acd-48af-cf04-003f64c18151",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:19.273231Z",
          "iopub.execute_input": "2025-02-03T14:45:19.273534Z",
          "iopub.status.idle": "2025-02-03T14:45:20.628579Z",
          "shell.execute_reply.started": "2025-02-03T14:45:19.273487Z",
          "shell.execute_reply": "2025-02-03T14:45:20.627712Z"
        },
        "colab": {
          "referenced_widgets": [
            "0ba62054400d4109b16ff6d30c8f571c",
            "825f604818b643bcb6ba5b018574af88"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ba62054400d4109b16ff6d30c8f571c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "825f604818b643bcb6ba5b018574af88"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas\n",
        "#pandas.DataFrame(dataset['train'])"
      ],
      "metadata": {
        "id": "rk2WHIJroTzq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:20.629567Z",
          "iopub.execute_input": "2025-02-03T14:45:20.629879Z",
          "iopub.status.idle": "2025-02-03T14:45:20.633537Z",
          "shell.execute_reply.started": "2025-02-03T14:45:20.629848Z",
          "shell.execute_reply": "2025-02-03T14:45:20.632857Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# To check a sample record from dataset\n",
        "dataset['train']['text'][10]"
      ],
      "metadata": {
        "id": "tMgBCg3ECRzz",
        "outputId": "4ac8cb4e-b368-450c-ecba-06dd2dd37e77",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:20.634391Z",
          "iopub.execute_input": "2025-02-03T14:45:20.634678Z",
          "iopub.status.idle": "2025-02-03T14:45:20.662015Z",
          "shell.execute_reply.started": "2025-02-03T14:45:20.634657Z",
          "shell.execute_reply": "2025-02-03T14:45:20.661243Z"
        }
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 03 Feb 2025\\n\\nYou are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\\n\\nAlways prioritize accuracy and clarity in your answers.\\nIf the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\\nProvide useful links or references to detailed documentation when appropriate.\\nUse a friendly and professional tone, ensuring the response is easy to understand.\\nIf the FAQ does not cover the question, offer an apology and suggest contacting customer support.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ni cannot notify of a error with a goddamn sign-up<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWe appreciate you reaching out to us to notify us about the error you encountered during the sign-up process. We understand that you may be frustrated and we apologize for any inconvenience caused. To assist you further, could you please provide us with more details about the issue you are facing? This will enable us to investigate and resolve the problem promptly. We value your feedback and look forward to resolving this matter for you.<|eot_id|>'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA (Low-Rank Adaptation) for fine-tuning the model on a language modeling task\n",
        "lora_config = LoraConfig(\n",
        "    r=4,                   # Rank for low-rank matrices\n",
        "    lora_alpha=8,         # Scaling factor\n",
        "    lora_dropout=0.2,      # Regularization dropout\n",
        "    task_type=\"CAUSAL_LM\"  # For language modeling\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "# Print the number of trainable parameters in the model after applying LoRA\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_WwZhdO9B88K",
        "outputId": "555d86e1-d4ee-48d7-8d2d-9ac55db3ae91",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:20.662749Z",
          "iopub.execute_input": "2025-02-03T14:45:20.662941Z",
          "iopub.status.idle": "2025-02-03T14:45:20.769211Z",
          "shell.execute_reply.started": "2025-02-03T14:45:20.662923Z",
          "shell.execute_reply": "2025-02-03T14:45:20.768439Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 1,146,880 || all params: 3,213,896,704 || trainable%: 0.0357\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up training arguments for the model training process\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Directory where the results will be saved\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    warmup_steps=5,  # Number of warmup steps to gradually increase the learning rate during training\n",
        "    learning_rate=2e-4,  # Learning rate for the optimizer\n",
        "    fp16=True,  # Enabling 16-bit floating point precision for faster training on GPUs that support it (reduces memory usage)\n",
        "    report_to=\"none\",  # Disabling logging/reporting to external services (e.g., TensorBoard, Weights & Biases)\n",
        ")\n",
        "\n",
        "# Initializing the SFTTrainer for supervised fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # The pre-trained model to be fine-tuned\n",
        "    train_dataset=dataset[\"train\"], # The dataset used for training\n",
        "    eval_dataset=dataset[\"test\"],  # The dataset used for validation\n",
        "    tokenizer=tokenizer,  # Tokenizer to process input text for the model\n",
        "    args=training_arguments,  # The training arguments defined above\n",
        "    peft_config=lora_config,\n",
        ")"
      ],
      "metadata": {
        "id": "NZiGAh5tHalx",
        "outputId": "5e20225d-4a53-463b-edf7-a95eddfea828",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:20.769952Z",
          "iopub.execute_input": "2025-02-03T14:45:20.770207Z",
          "iopub.status.idle": "2025-02-03T14:45:22.266254Z",
          "shell.execute_reply.started": "2025-02-03T14:45:20.770186Z",
          "shell.execute_reply": "2025-02-03T14:45:22.265308Z"
        },
        "colab": {
          "referenced_widgets": [
            "f3f348506cc8470e8c08f68af6ee3f13",
            "4a417699b220481cb260bdb0854a0c9c"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-18-7e50a1da9974>:14: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3f348506cc8470e8c08f68af6ee3f13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/200 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a417699b220481cb260bdb0854a0c9c"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8yyOfG0dHcrq",
        "outputId": "1d6503ff-03a7-4b67-a666-79fb2beb399e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:45:22.267013Z",
          "iopub.execute_input": "2025-02-03T14:45:22.267346Z",
          "iopub.status.idle": "2025-02-03T14:51:07.844522Z",
          "shell.execute_reply.started": "2025-02-03T14:45:22.267316Z",
          "shell.execute_reply": "2025-02-03T14:51:07.843773Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 05:43, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.678900</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=800, training_loss=0.6205354309082032, metrics={'train_runtime': 345.104, 'train_samples_per_second': 2.318, 'train_steps_per_second': 2.318, 'total_flos': 4343979448553472.0, 'train_loss': 0.6205354309082032, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "ZCcLZ6OzIRVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response based on the input prompt\n",
        "def generate(input_prompt):\n",
        "    # Define the system and user messages to provide context for the conversation\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},  # System message with the pre-defined instructions\n",
        "        {\"role\": \"user\", \"content\": input_prompt}   # User's input prompt\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template to format the messages, without tokenizing yet, and add the generation prompt\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize the formatted prompt, padding and truncating as necessary, and move the data to the GPU\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "    # Generate the model's output based on the tokenized input, limiting to a maximum of 2048 new tokens\n",
        "    outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
        "\n",
        "    # Decode the output tokens back into text, skipping any special tokens (like padding)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return text  # Return the generated text"
      ],
      "metadata": {
        "id": "PNrrXfHFJCH5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:07.845366Z",
          "iopub.execute_input": "2025-02-03T14:51:07.845646Z",
          "iopub.status.idle": "2025-02-03T14:51:07.850272Z",
          "shell.execute_reply.started": "2025-02-03T14:51:07.845623Z",
          "shell.execute_reply": "2025-02-03T14:51:07.849547Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate(\"Where to see what payment options are available?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "lNTHKpXsJOJJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:07.851052Z",
          "iopub.execute_input": "2025-02-03T14:51:07.851298Z",
          "iopub.status.idle": "2025-02-03T14:51:16.886164Z",
          "shell.execute_reply.started": "2025-02-03T14:51:07.85128Z",
          "shell.execute_reply": "2025-02-03T14:51:16.885466Z"
        },
        "outputId": "ecc80aaa-214d-49c2-c1cb-82f7f0e7ba1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 03 Feb 2025\n\nYou are a helpful and efficient customer support bot designed to assist users by providing answers to frequently asked questions (FAQs) related to our products and services. Your responses should be concise, clear, and friendly, ensuring the user feels heard and supported. If the user’s question is outside the scope of the FAQ, gently direct them to contact customer support.\n\nAlways prioritize accuracy and clarity in your answers.\nIf the user asks a complex question, break it down into smaller, manageable parts and answer step-by-step.\nProvide useful links or references to detailed documentation when appropriate.\nUse a friendly and professional tone, ensuring the response is easy to understand.\nIf the FAQ does not cover the question, offer an apology and suggest contacting customer support.user\n\nWhere to see what payment options are available?assistant\n\nI'm here to assist you! I understand your curiosity about where you can find information about the available payment options. To explore the different payment options, you can visit our website and navigate to the \"Payment Methods\" or \"Checkout\" section. On this page, you'll find a comprehensive list of the various payment options we offer, including credit cards, PayPal, and other secure payment gateways. If you have any specific questions or need further assistance, please let me know. I'm here to help you every step of the way!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# to format output\n",
        "print(response.split(\"assistant\")[-1])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:16.886896Z",
          "iopub.execute_input": "2025-02-03T14:51:16.887129Z",
          "iopub.status.idle": "2025-02-03T14:51:16.89138Z",
          "shell.execute_reply.started": "2025-02-03T14:51:16.88711Z",
          "shell.execute_reply": "2025-02-03T14:51:16.890569Z"
        },
        "id": "cT-8xlKMebty",
        "outputId": "c2b4b4de-c838-4e10-9a82-ecd67879b0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\nI'm here to assist you! I understand your curiosity about where you can find information about the available payment options. To explore the different payment options, you can visit our website and navigate to the \"Payment Methods\" or \"Checkout\" section. On this page, you'll find a comprehensive list of the various payment options we offer, including credit cards, PayPal, and other secure payment gateways. If you have any specific questions or need further assistance, please let me know. I'm here to help you every step of the way!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "DQ4pgMd8K7Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:16.892259Z",
          "iopub.execute_input": "2025-02-03T14:51:16.892545Z",
          "iopub.status.idle": "2025-02-03T14:51:16.911585Z",
          "shell.execute_reply.started": "2025-02-03T14:51:16.892511Z",
          "shell.execute_reply": "2025-02-03T14:51:16.910911Z"
        },
        "id": "6xg27oAQa8MN",
        "outputId": "5eec79a1-35e7-402a-fbd7-5270132cd862"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/customer-faq-llama-3.2-3B\") # Saves the model under the same directory."
      ],
      "metadata": {
        "id": "huyCLVJeK24R",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:16.912249Z",
          "iopub.execute_input": "2025-02-03T14:51:16.912499Z",
          "iopub.status.idle": "2025-02-03T14:51:17.424497Z",
          "shell.execute_reply.started": "2025-02-03T14:51:16.912478Z",
          "shell.execute_reply": "2025-02-03T14:51:17.423814Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# To push the model to hugginface\n",
        "model.push_to_hub(\"customer-faq-llama-3.2-3B\")"
      ],
      "metadata": {
        "id": "30GUwMjbMYeA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:17.427171Z",
          "iopub.execute_input": "2025-02-03T14:51:17.427388Z",
          "iopub.status.idle": "2025-02-03T14:51:20.58039Z",
          "shell.execute_reply.started": "2025-02-03T14:51:17.427369Z",
          "shell.execute_reply": "2025-02-03T14:51:20.579473Z"
        },
        "outputId": "5a127c76-bdfe-4b9b-d8de-d400c32a3a53",
        "colab": {
          "referenced_widgets": [
            "391f2a36de7b47599d7937e70c1be405",
            "a7ca4973462445679b292f0ba339c7f3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "391f2a36de7b47599d7937e70c1be405"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/4.60M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7ca4973462445679b292f0ba339c7f3"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Shameer-Khan/customer-faq-llama-3.2-3B/commit/5ca07fe0c14267a50a9fdf6cf0257e62bcbd9aaf', commit_message='Upload model', commit_description='', oid='5ca07fe0c14267a50a9fdf6cf0257e62bcbd9aaf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Shameer-Khan/customer-faq-llama-3.2-3B', endpoint='https://huggingface.co', repo_type='model', repo_id='Shameer-Khan/customer-faq-llama-3.2-3B'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model would be saved like this: https://huggingface.co/Shameer-Khan/customer-faq-llama-3.2-3B"
      ],
      "metadata": {
        "id": "zaNRAre1NWWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge LoRa adapter model with Base model"
      ],
      "metadata": {
        "id": "xEuPIWl0a8MO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load models"
      ],
      "metadata": {
        "id": "MOpq72QtLFK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load locally\n",
        "tuned_local_model = AutoModelForCausalLM.from_pretrained(\"/content/customer-faq-llama-3.2-3B\")\n",
        "\n",
        "# load the saved model from huggingface\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"Shameer-Khan/customer-faq-llama-3.2-3\",  # Name of the base model defined earlier\n",
        "#     device_map=\"auto\",  # Automatically map model layers to available devices (e.g., GPU/CPU)\n",
        "#     quantization_config=bnb_config,  # Apply the defined 4-bit quantization configuration\n",
        "# )"
      ],
      "metadata": {
        "id": "ZIeqOROTLC8C",
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:20.581706Z",
          "iopub.execute_input": "2025-02-03T14:51:20.582061Z",
          "iopub.status.idle": "2025-02-03T14:51:27.911156Z",
          "shell.execute_reply.started": "2025-02-03T14:51:20.582032Z",
          "shell.execute_reply": "2025-02-03T14:51:27.910287Z"
        },
        "outputId": "134f3f55-ad0c-4e5a-e06f-7d4854a001a4",
        "colab": {
          "referenced_widgets": [
            "8d408536fb4446d484448a88021f5ee4"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d408536fb4446d484448a88021f5ee4"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(base_model, low_cpu_mem_usage=True,\n",
        "                                                  return_dict=True, device_map=\"auto\",\n",
        "                                                  quantization_config=bnb_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:27.911951Z",
          "iopub.execute_input": "2025-02-03T14:51:27.912242Z",
          "iopub.status.idle": "2025-02-03T14:51:36.063645Z",
          "shell.execute_reply.started": "2025-02-03T14:51:27.912218Z",
          "shell.execute_reply": "2025-02-03T14:51:36.062958Z"
        },
        "id": "aemAMiiia8MP",
        "outputId": "456857a1-6e97-40b2-9227-9481af1b2778",
        "colab": {
          "referenced_widgets": [
            "b636ee53af724aeb9610892223467870"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b636ee53af724aeb9610892223467870"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model=base_model, model_id=\"/content/customer-faq-llama-3.2-3B\",\n",
        "                                  quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "\n",
        "model = model.merge_and_unload(safe_merge=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T14:51:36.064387Z",
          "iopub.execute_input": "2025-02-03T14:51:36.064684Z",
          "iopub.status.idle": "2025-02-03T14:51:37.762356Z",
          "shell.execute_reply.started": "2025-02-03T14:51:36.06465Z",
          "shell.execute_reply": "2025-02-03T14:51:37.761592Z"
        },
        "id": "3TSuDR0va8MQ",
        "outputId": "bac0de5b-50cd-4828-b2b2-1a1a82da9283"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}